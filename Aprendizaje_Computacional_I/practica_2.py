import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""
Section 0

The following parameters should be modified only when indicated in the practice document.
"""
SEED = 77384961                             # Random seed. This must be changed to your DNI/Passport number.
TEST_SPLIT = 0.1                            # Share of data to be used to test.
FOLDER_TO_SAVE_IMAGES= "./output_images"    # Where will be saved each image generated by the script.
NUMBER_OF_FOLDS = 5                         # K value when applying K-Fold.

if not os.path.exists(FOLDER_TO_SAVE_IMAGES):
    os.makedirs(FOLDER_TO_SAVE_IMAGES)

"""
Section 1
LOAD DATA FOR CLASSIFICATION

Load breast cancer dataset included in scikit learn. This is a binary dataset, so the label feature (cancer Yes o cancer No) can only be 1 or 0.
Follow the following example to load the data from sklearn.
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer
Remember to load it as a Pandas dataframe and with (data, labels) format.
"""

from sklearn.datasets import load_breast_cancer
data, labels = load_breast_cancer(return_X_y=True, as_frame=True)

print("List of features included in the dataset:")
print(list(data.columns))
print("The table with the data looks as follows:")
print(data)
print("Labels vector must includes a binary clasification for each of them.")
print(f"We have a total of {data.shape[0]} examples and {labels.shape[0]} labels with values {labels.unique()}.")

"""
Section 2
CLEAN DATA FOR CLASSIFICATION
Now data should be cleaned (discard or fill missing data, use some technique to balance the amount of examples for each label, etc).
In this exercies, we will use only the feature "mean perimeter", "mean smoothness" and "mean concave points" to train. Filter data columns to leave only that one in dataframe.
"""

data = data[['mean perimeter', 'mean smoothness', 'mean concave points']]

"""
Since this time a Decision Tree will be used to train and they don't work well with continious data, we will discretize the information.
Use https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer
to discretize the information into 10 bins with ordinal encoding.
"""

#
from sklearn.preprocessing import KBinsDiscretizer
enc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')
data_binned = enc.fit_transform(data)
print(data)
print(data_binned)

"""
SECTION 3
SPLIT THE DATA, TRAIN AND TEST FOR CLASSIFICATION
Accuracy is not always the best metric to measure how good a trained model is, so this time we will obtain some other measures.
Following the information from https://en.wikipedia.org/wiki/Precision_and_recall, obtain Recall, Precision and False Negative Rate.

"""

def get_metrics_classification(true_labels, pred_labels):

    TP = np.logical_and(pred_labels==1, true_labels==1).sum()
    TN = np.logical_and(pred_labels==0, true_labels==0).sum()
    FP = np.logical_and(pred_labels==1, true_labels==0).sum()
    FN = np.logical_and(pred_labels==0, true_labels==1).sum()

    accuracy = (TP+TN)/(TP+TN+FP+FN)
    recall = TP/(TP+FN) if (TP + FN) > 0 else 0 #He comprobado si el divisor es mayor que 0 aunque si no te daría NaN en la sección y sabrías que es por esto.
    precision = TP/(TP+FP) if (TP + FP) > 0 else 0
    false_negative_rate = FN/(TP+FN) if (TP + FN) > 0 else 0

    return accuracy, recall, precision, false_negative_rate

"""
Cross Validation
In order to evaluate a training approach, the division of data into training and test subsets can lead to erroneous conclusions due to chance.
In order to make the conclusions more reliable, there is the Cross Validation technique. In this technique, several training and test divisions are created,
training is performed, metrics are obtained for each of them and finally an average of the metrics obtained is made.

Use KFold to implement your own cross validation for a given model instance.
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html
"""
from sklearn.model_selection import KFold

def cross_validation_classification(model, data, labels, K, seed):

    kf = KFold(n_splits=K, shuffle=True, random_state=seed)

    acc_list = []
    recall_list = []
    precision_list = []
    fnr_list = []

    for i, (train_index, test_index) in enumerate(kf.split(data)):
        print(f"Fold {i}")
        # Get the train and splits data and labels.
        train_data = data[train_index]
        test_data = data[test_index]
        train_labels = labels[train_index]
        test_labels = labels[test_index]


        # Fit the model.
        model.fit(train_data, train_labels)

        # Predict.
        test_prediction = model.predict(test_data)
        # Remember to turn test_prediction and test_labels into numpy arrays if they are not.
        test_prediction = np.array(test_prediction)
        test_labels = np.array(test_labels)

        # Include each obtained metric into the according list.
        accuracy, recall, precision, fnr = get_metrics_classification(test_labels, test_prediction)
        acc_list.append(accuracy)
        recall_list.append(recall)
        precision_list.append(precision)
        fnr_list.append(fnr)

        print(f"Accuracy={accuracy}, recall={recall}, precision={precision} and FNR={fnr}.")

        """
        We will save a confusion matric for each training.
        """
        cm = confusion_matrix(test_labels, test_prediction, labels=[0,1])
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])
        disp.plot()
        plt.savefig(f"{FOLDER_TO_SAVE_IMAGES}/confusion_matrix_practice_2_fold_{i}_seed_{SEED}.png")

    # Obtain the average for each list of metrics.
    average_accuracy = np.mean(acc_list)
    average_recall = np.mean(recall_list)
    average_precision = np.mean(precision_list)
    average_fnr = np.mean(fnr_list)

    print(f"Provided model with cross validation K={K} gets accuracy={average_accuracy}, recall={average_recall}, precision={average_precision} and FNR={average_fnr}.")

    return average_accuracy, average_recall, average_precision, average_fnr

"""
This time we will use Decision Trees as model.
Instantiate the decision tree to apply cross validation following
https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
Remember to provide SEED as random_state.
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state = SEED)

cross_validation_classification(dt, data_binned, labels, NUMBER_OF_FOLDS, SEED)

"""
Since we are currently training decision trees, it's useful to obtain a visual representation for the tree.

As OPTIONAL exercise, you could implement the visualization of a decision tree following https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html
"""
from sklearn.tree import plot_tree

plt.figure(figsize=(30, 20))
plot_tree(dt, filled=True, rounded=True)
plt.savefig(f"{FOLDER_TO_SAVE_IMAGES}/decision_tree_visualization.png")
plt.show()

"""
SECTION 4

LOAD DATA FOR REGRESSION

Load the Diabetes dataset included in scikit learn. This is a regression dataset,
where the output feature is a number between 25 and 346.
Follow the following example to load the data from sklearn.
https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes
Remember to load it as a Pandas dataframe and with (data, labels) format.

"""

from sklearn.datasets import load_diabetes

# Load the diabetes dataset
diabetes_X, diabetes_y = load_diabetes(return_X_y=True)

"""
SECTION 5
SPLIT THE DATA, TRAIN AND TEST FOR REGRESSION
You must compute the following regression performance measures: Mean Squared Error (MSE),
Mean Absolute Error (MAE), Median Absolute Error, and Coefficient of determination (R2)

"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score

def get_metrics_regression(true_outputs, pred_outputs):

    my_mean_squared_error = mean_squared_error(true_outputs, pred_outputs)
    my_mean_absolute_error = mean_absolute_error(true_outputs, pred_outputs)
    my_median_absolute_error = median_absolute_error(true_outputs, pred_outputs)
    my_r2_score = r2_score(true_outputs, pred_outputs)

    return my_mean_squared_error, my_mean_absolute_error, my_median_absolute_error, my_r2_score

"""
Cross Validation
You must adapt your previous code for k-fold cross-validation to this regression
problem.
"""

def cross_validation_regression(model, data, outputs, K, seed):

    kf = KFold(n_splits=K, shuffle=True, random_state=seed)

    mse_list = []
    mae_list = []
    medae_list = []
    r2_list = []

    for i, (train_index, test_index) in enumerate(kf.split(data)):
        print(f"Fold {i}")
        # Get the train and splits data and labels.
        train_data = data[train_index]
        test_data = data[test_index]
        train_outputs = outputs[train_index]
        test_outputs = outputs[test_index]

        # Fit the model.
        model.fit(train_data, train_outputs)

        # Predict.
        test_prediction = model.predict(test_data)
        # Remember to turn test_prediction into a numpy array if it is not.
        test_prediction = np.array(test_prediction)

        # Include each obtained metric into the according list.
        my_mse, my_mae, my_medae, my_r2 = get_metrics_regression(test_outputs, test_prediction)
        mse_list.append(my_mse)
        mae_list.append(my_mae)
        medae_list.append(my_medae)
        r2_list.append(my_r2)

        print(f"MSE={my_mse}, MAE={my_mae}, MedSE={my_medae} and R2={my_r2}.")


    # Obtain the average for each list of metrics.
    average_mse = np.mean(mse_list)
    average_mae = np.mean(mae_list)
    average_medae = np.mean(medae_list)
    average_r2 = np.mean(r2_list)

    print(f"Provided model with cross validation K={K} gets MSE={average_mse}, MAE={average_mae}, MedAE={average_medae} and R2={average_r2}.")

    return average_mse, average_mae, average_medae, average_r2

"""
This time we will use linear regression with stochastic gradient descent as model.
Instantiate the linear regression to apply cross validation following
https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.SGDRegressor.html
Remember to provide SEED as random_state.
"""

from sklearn.linear_model import SGDRegressor

lr = SGDRegressor(random_state=SEED, max_iter = 10000)

cross_validation_regression(lr, diabetes_X, diabetes_y, NUMBER_OF_FOLDS, SEED)

"""
Aquí lo que voy a hacer es calcular el máximo de profundidad basándome en el recall porque lo que queremos es que haya la menor cantidad de 
falsos negativos ya que como es cáncer queremos que si tiene cáncer que se detecte en la mayoría de los casos aunque eso supongo aumentar
falsos positivos
"""
def cross_validation_classification_max_recall(model, data, labels, K, seed):
    kf = KFold(n_splits=K, shuffle=True, random_state=seed)

    recall_list = []

    for fold_idx, (train_index, test_index) in enumerate(kf.split(data)):
        train_data, test_data = data[train_index], data[test_index]
        train_labels, test_labels = labels[train_index], labels[test_index]

        # Fit the model
        model.fit(train_data, train_labels)

        # Predict
        test_prediction = model.predict(test_data)

        # Calculate recall
        tp = np.sum((test_labels == 1) & (test_prediction == 1))
        fn = np.sum((test_labels == 1) & (test_prediction == 0))
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero
        recall_list.append(recall)

        # Print recall for each fold
        print(f"Fold {fold_idx + 1}, Recall: {recall:.4f}")

    # Calculate average recall
    average_recall = np.mean(recall_list)
    return average_recall


# Depth tuning to maximize recall
depth_values = np.linspace(1, 20, 20).astype(int)
best_depth = None
best_recall = -1  # Start with the lowest possible recall
recall_results = []

for d in depth_values:
    dt = DecisionTreeClassifier(random_state=SEED, max_depth=d)
    print(f"\nEvaluating depth: {d}")
    average_recall = cross_validation_classification_max_recall(
        dt, data_binned, labels, NUMBER_OF_FOLDS, SEED
    )
    recall_results.append(average_recall)

    print(f"Depth {d}, Average Recall: {average_recall:.4f}")

    if average_recall > best_recall:
        best_recall = average_recall
        best_depth = d

print(f"\nBest depth to maximize recall is {best_depth} with recall={best_recall:.4f}")

def cross_validation_regression_with_alpha(data, outputs, K, seed, alpha_values):
    """
    Optimiza el hiperparámetro alpha para un modelo de regresión lineal con regularización L2 (usando SGDRegressor).
    """
    kf = KFold(n_splits=K, shuffle=True, random_state=seed)
    best_alpha = None
    best_score = float('inf')
    results = {}

    for alpha in alpha_values:
        mse_list = []
        for train_index, test_index in kf.split(data):
            # Dividimos en conjuntos de entrenamiento y prueba
            train_data, test_data = data[train_index], data[test_index]
            train_outputs, test_outputs = outputs[train_index], outputs[test_index]

            # Creamos el modelo con el valor actual de alpha
            model = SGDRegressor(alpha=alpha, random_state=seed, max_iter=10000)

            # Ajustamos el modelo
            model.fit(train_data, train_outputs)

            # Realizamos predicciones
            test_prediction = model.predict(test_data)

            # Calculamos el MSE y lo almacenamos
            mse = mean_squared_error(test_outputs, test_prediction)
            mse_list.append(mse)

        # Calculamos el promedio de MSE para este alpha
        avg_mse = np.mean(mse_list)
        results[alpha] = avg_mse

        # Actualizamos el mejor alpha si tiene menor MSE
        if avg_mse < best_score:
            best_score = avg_mse
            best_alpha = alpha

    print(f"Best alpha: {best_alpha} with MSE={best_score}")
    return results, best_alpha

# Valores de alpha a probar
alpha_values = [0.00001, 0.000000001,0.01, 0.1, 1, 10, 100] #Llegamos a la conclusión de que cuanto más pequeño el alpha mejor

# Llamada a la función para optimizar alpha
results, best_alpha = cross_validation_regression_with_alpha(diabetes_X, diabetes_y, NUMBER_OF_FOLDS, SEED, alpha_values)
